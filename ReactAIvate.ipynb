{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd64deb7-ae0f-413d-9a68-dc916fdb0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dgllife.model import model_zoo\n",
    "from dgllife.utils import smiles_to_bigraph\n",
    "from dgllife.utils import EarlyStopping, Meter\n",
    "from dgllife.utils import AttentiveFPAtomFeaturizer\n",
    "from dgllife.utils import AttentiveFPBondFeaturizer\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from IPython.display import SVG, display\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import argparse\n",
    "from rdkit import RDLogger \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RDLogger.DisableLog('rdApp.*') # switch off RDKit warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "911c5f86-3ee9-4017-af62-0dc30248c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_values_at_positions, atom_finder, smiles_augmentation, concat_feature_reactive_atom, collate_molgraphs, Canon_SMILES_similarity\n",
    "from model import AttentiveFPPredictor_rxn, weighted_binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e44e043-3364-4093-8280-b4e714030ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in graph :  39\n"
     ]
    }
   ],
   "source": [
    "atom_featurizer = AttentiveFPAtomFeaturizer(atom_data_field='hv')\n",
    "bond_featurizer = AttentiveFPBondFeaturizer(bond_data_field='he')\n",
    "n_feats = atom_featurizer.feat_size('hv')\n",
    "e_feats = bond_featurizer.feat_size('he')\n",
    "print( 'Number of features in graph : ' , n_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7149fd03-67e1-48a7-88d3-d995f2031e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign device \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2367f09c-e744-45b2-be49-cde0c29ca20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>atom_mapped</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC(C)(C)[P+]([Pd])(c1ccnn1-c1c(-c2ccccc2)nn(-c...</td>\n",
       "      <td>[5, 39, 38]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COc1ccc([Pd](Br)[P+](c2nnn(-c3ccccc3)c2-c2c(OC...</td>\n",
       "      <td>[56, 57, 51, 52]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COc1cc(F)cc([Pd](c2cn(S(=O)(=O)c3ccccc3)c3cccc...</td>\n",
       "      <td>[7, 8, 9]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COc1cccc([Pd](N2CCC(O)CC2)[P+](c2ccc(N3CCN(C)C...</td>\n",
       "      <td>[6, 7, 8]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC(C)CN1CCN2CCN(CC(C)C)P1(=N[P+](C1CCCCC1)(C1C...</td>\n",
       "      <td>[70, 71, 60, 61]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>CNCc1cccc([Pd](Cl)[P+](c2ccc3ccccc3c2-c2cccc3c...</td>\n",
       "      <td>[8, 9, 44, 41]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>CC(C)(C)[P+]([Pd](Br)c1cnc2ccccc2c1)(C(C)(C)C)...</td>\n",
       "      <td>[5, 28]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>CC(C)Oc1cccc(OC(C)C)c1-c1ccccc1[P+](C1CCCCC1)(...</td>\n",
       "      <td>[59, 60, 54, 55]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>CCN(CC)c1ccc([P+]([Pd](Cl)c2ccccc2C#N)([C@]23C...</td>\n",
       "      <td>[10, 46]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>COc1cccc([Pd](c2cccc(S(C)(=O)=O)c2)[P+](c2cccc...</td>\n",
       "      <td>[6, 7, 8]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  smiles       atom_mapped  \\\n",
       "0      CC(C)(C)[P+]([Pd])(c1ccnn1-c1c(-c2ccccc2)nn(-c...       [5, 39, 38]   \n",
       "1      COc1ccc([Pd](Br)[P+](c2nnn(-c3ccccc3)c2-c2c(OC...  [56, 57, 51, 52]   \n",
       "2      COc1cc(F)cc([Pd](c2cn(S(=O)(=O)c3ccccc3)c3cccc...         [7, 8, 9]   \n",
       "3      COc1cccc([Pd](N2CCC(O)CC2)[P+](c2ccc(N3CCN(C)C...         [6, 7, 8]   \n",
       "4      CC(C)CN1CCN2CCN(CC(C)C)P1(=N[P+](C1CCCCC1)(C1C...  [70, 71, 60, 61]   \n",
       "...                                                  ...               ...   \n",
       "99995  CNCc1cccc([Pd](Cl)[P+](c2ccc3ccccc3c2-c2cccc3c...    [8, 9, 44, 41]   \n",
       "99996  CC(C)(C)[P+]([Pd](Br)c1cnc2ccccc2c1)(C(C)(C)C)...           [5, 28]   \n",
       "99997  CC(C)Oc1cccc(OC(C)C)c1-c1ccccc1[P+](C1CCCCC1)(...  [59, 60, 54, 55]   \n",
       "99998  CCN(CC)c1ccc([P+]([Pd](Cl)c2ccccc2C#N)([C@]23C...          [10, 46]   \n",
       "99999  COc1cccc([Pd](c2cccc(S(C)(=O)=O)c2)[P+](c2cccc...         [6, 7, 8]   \n",
       "\n",
       "       class_label  \n",
       "0                0  \n",
       "1                1  \n",
       "2                3  \n",
       "3                3  \n",
       "4                1  \n",
       "...            ...  \n",
       "99995            2  \n",
       "99996            4  \n",
       "99997            1  \n",
       "99998            4  \n",
       "99999            3  \n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_elementary = pd.read_csv('elementary_step_100000.csv')\n",
    "df_elementary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316d536b-4113-4902-99b6-12df80613a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets_, test_datasets = train_test_split( df_elementary, test_size=0.2, random_state=42, shuffle = False)\n",
    "train_datasets, valid_datasets = train_test_split( train_datasets_, test_size=0.125, random_state=42, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37868a08-f674-40f9-b856-56c0134f74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datasets = valid_datasets.reset_index(drop=True)\n",
    "test_datasets = test_datasets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baec6850-8a36-47c9-8ee0-08f759a8e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augm_smiles = smiles_augmentation(train_datasets, 5, augmentation =False)\n",
    "valid_augm_smiles = smiles_augmentation(valid_datasets, 5, augmentation =False)\n",
    "test_augm_smiles = smiles_augmentation(test_datasets, 5, augmentation =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f16c6e-9bce-4651-9dfb-3bcd4f237b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reaction steps after SMILES augmentation :  100000\n"
     ]
    }
   ],
   "source": [
    "print ( 'Total number of reaction steps after SMILES augmentation : ', len(train_augm_smiles) + len(valid_augm_smiles)+ len(test_augm_smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ef5c13e-0aa3-469c-9ea0-601744050ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_generation(df_augm_smiles):    \n",
    "    graph_for_rxn = []\n",
    "    for i in range(len(df_augm_smiles)):\n",
    "        graph_for_rxn.append(smiles_to_bigraph(df_augm_smiles[i][0], node_featurizer=atom_featurizer,edge_featurizer=bond_featurizer, canonical_atom_order=False))\n",
    "    return graph_for_rxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "739fdc8a-c345-4e67-9bd9-756b8773008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_for_rxn = graph_generation(train_augm_smiles)\n",
    "valid_graph_for_rxn = graph_generation(valid_augm_smiles)\n",
    "test_graph_for_rxn = graph_generation(test_augm_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "463430e4-a0ba-4284-baa7-4b0f9778d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_dataset = concat_feature_reactive_atom(train_graph_for_rxn, train_augm_smiles)\n",
    "valid_graph_dataset = concat_feature_reactive_atom(valid_graph_for_rxn, valid_augm_smiles)\n",
    "test_graph_dataset = concat_feature_reactive_atom(test_graph_for_rxn, test_augm_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a3e5c2c-e42d-45b8-8917-1f7cc60721b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_graph_dataset, batch_size=256,shuffle=False,\n",
    "                          collate_fn=collate_molgraphs)\n",
    "valid_loader = DataLoader(valid_graph_dataset, batch_size=256,shuffle=False,\n",
    "                          collate_fn=collate_molgraphs)\n",
    "test_loader = DataLoader(test_graph_dataset, batch_size=256,shuffle=False,\n",
    "                          collate_fn=collate_molgraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "630422cc-b1d0-441b-bb6b-61d76626ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of distribution dataloader preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ee7f474-8f14-4c5e-a9ea-d3b6d887c76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 smiles       atom_mapped  \\\n",
      "0     [Pd][P+](c1ccccc1)(c1ccccc1)c1ccccc1.COC(=O)c1...       [0, 29, 30]   \n",
      "1     COC(=O)c1cncc2c([Pd](Br)[P+](c3ccccc3)(c3ccccc...  [47, 48, 42, 43]   \n",
      "2     COC(=O)c1cncc2c([Pd](Br)[P+](c3ccccc3)(c3ccccc...  [10, 11, 42, 43]   \n",
      "3     COC(=O)c1cncc2c([Pd](c3cccc(OC)c3)[P+](c3ccccc...       [9, 10, 11]   \n",
      "4     [Pd][P+](c1ccccc1)(c1ccccc1)c1ccccc1.Cc1cc(C(=...       [0, 31, 32]   \n",
      "...                                                 ...               ...   \n",
      "3642  FC(F)(F)c1ccc([Pd](Cl)[P+](C2CCCCC2)(C2CCCCC2)...    [8, 9, 35, 36]   \n",
      "3643  C[Pd](c1ccc(C(F)(F)F)cc1C(Cl)(Cl)Cl)[P+](C1CCC...         [0, 1, 2]   \n",
      "3644  CC(C)(C)[P+]([Pd])(c1cc2ccccc2[cH-]1)C(C)(C)C....       [5, 32, 33]   \n",
      "3645  Cc1cc(C(=O)O)cc(S(=O)(=O)F)c1[Pd](Br)[P+](c1cc...  [14, 15, 38, 39]   \n",
      "3646  Cc1ccc([Pd](c2c(C)cc(C(=O)O)cc2S(=O)(=O)F)[P+]...         [4, 5, 6]   \n",
      "\n",
      "      class_label  \n",
      "0               0  \n",
      "1               1  \n",
      "2               2  \n",
      "3               3  \n",
      "4               0  \n",
      "...           ...  \n",
      "3642            6  \n",
      "3643            3  \n",
      "3644            0  \n",
      "3645            6  \n",
      "3646            3  \n",
      "\n",
      "[3647 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df_ood_1000 = pd.read_csv('OOD_elementary_step_3647.csv')\n",
    "print(df_ood_1000)\n",
    "ood_augm_smiles = smiles_augmentation(df_ood_1000, 5, augmentation =False)\n",
    "ood_graph_for_rxn = graph_generation(ood_augm_smiles)\n",
    "ood_graph_dataset = concat_feature_reactive_atom(ood_graph_for_rxn, ood_augm_smiles)\n",
    "ood_loader = DataLoader(ood_graph_dataset, batch_size=256,shuffle=False,\n",
    "                          collate_fn=collate_molgraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccbfba61-4b9b-4409-a45a-5f682645a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the model to fit your classification task\n",
    "model = AttentiveFPPredictor_rxn(node_feat_size=n_feats,\n",
    "                                   edge_feat_size=e_feats,\n",
    "                                   num_layers=2,\n",
    "                                   num_timesteps=1,\n",
    "                                   graph_feat_size=200,\n",
    "                                   n_tasks=8,\n",
    "                                   dropout=0.1\n",
    "                                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9210e1d-bf53-4248-8d5c-aba3ee884dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentiveFPPredictor_rxn(\n",
       "  (gnn): AttentiveFPGNN(\n",
       "    (init_context): GetContext(\n",
       "      (project_node): Sequential(\n",
       "        (0): Linear(in_features=39, out_features=200, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (project_edge1): Sequential(\n",
       "        (0): Linear(in_features=49, out_features=200, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (project_edge2): Sequential(\n",
       "        (0): Dropout(p=0.1, inplace=False)\n",
       "        (1): Linear(in_features=400, out_features=1, bias=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (attentive_gru): AttentiveGRU1(\n",
       "        (edge_transform): Sequential(\n",
       "          (0): Dropout(p=0.1, inplace=False)\n",
       "          (1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (gru): GRUCell(200, 200)\n",
       "      )\n",
       "    )\n",
       "    (gnn_layers): ModuleList(\n",
       "      (0): GNNLayer(\n",
       "        (project_edge): Sequential(\n",
       "          (0): Dropout(p=0.1, inplace=False)\n",
       "          (1): Linear(in_features=400, out_features=1, bias=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (attentive_gru): AttentiveGRU2(\n",
       "          (project_node): Sequential(\n",
       "            (0): Dropout(p=0.1, inplace=False)\n",
       "            (1): Linear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "          (gru): GRUCell(200, 200)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (readout): AttentiveFPReadout(\n",
       "    (readouts): ModuleList(\n",
       "      (0): GlobalPool(\n",
       "        (compute_logits): Sequential(\n",
       "          (0): Linear(in_features=400, out_features=1, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (project_nodes): Sequential(\n",
       "          (0): Dropout(p=0.1, inplace=False)\n",
       "          (1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (gru): GRUCell(200, 200)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (predict): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=200, out_features=8, bias=True)\n",
       "  )\n",
       "  (node_predict): Sequential(\n",
       "    (0): Linear(in_features=200, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca198aa3-b897-4f12-b84a-cf8f90564d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "loss_fn_graph = nn.CrossEntropyLoss()\n",
    "loss_fn_node = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a60be92a-d9b2-49d9-88ee-cb0c78de7161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_a_train_epoch(n_epochs, epoch, model, data_loader, loss_criterion1, loss_criterion2, optimizer):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    loss_node_app = []\n",
    "    loss_graph_radomize_app = []\n",
    "    y_true_node = []\n",
    "    y_pred_node = []\n",
    "    \n",
    "    for batch_id, batch_data in enumerate(data_loader):\n",
    "        \n",
    "        smiles, bg, labels = batch_data\n",
    "        \n",
    "        bg = bg.to(device)\n",
    "        labels = labels.to(device)\n",
    "        n_feats_w_l = bg.ndata.pop('hv').to(device)\n",
    "        e_feats_ = bg.edata.pop('he').to(device)\n",
    "        n_feats_ = n_feats_w_l[:,:n_feats]\n",
    "        prediction1, prediction2, graph_feat = model(bg, n_feats_, e_feats_)\n",
    "        n_labels = n_feats_w_l[:,n_feats].unsqueeze(1)\n",
    "    \n",
    "        # Calculate the weights\n",
    "        counts = torch.bincount(n_labels.view(-1).long())\n",
    "        class_weights = 1.0 / counts.float()\n",
    "        class_weights = class_weights / class_weights.sum()\n",
    "    \n",
    "        \n",
    "        loss_graph = loss_fn_graph(prediction1, labels.squeeze(1).long())\n",
    "        loss_node = weighted_binary_cross_entropy(prediction2,n_labels ,class_weights)\n",
    "        loss = loss_graph + loss_node \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss_graph.data.item())\n",
    "        loss_node_app.append(loss_node.data.item())\n",
    "        \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(prediction1.detach().cpu().numpy())\n",
    "    \n",
    "        y_true_node.extend(n_labels.cpu().numpy())\n",
    "        y_pred_node.extend(prediction2.detach().cpu().numpy())\n",
    "\n",
    "    \n",
    "    total_loss = np.mean(losses)\n",
    "    total_loss_node = np.mean(loss_node_app)\n",
    "    total_loss_graph_random = np.mean(loss_graph_radomize_app)\n",
    "    accuracy = accuracy_score(y_true, np.argmax(y_pred, axis=1))\n",
    "    print('F1 score classification task:', f1_score(y_true,np.argmax(y_pred, axis=1), average='macro'))\n",
    "\n",
    "    # Threshold for binary prediction\n",
    "    threshold_1 = 0.5\n",
    "    # Convert predicted probabilities to binary values\n",
    "    y_pred_binary = [1 if pred >= threshold_1 else 0 for pred in np.concatenate(y_pred_node)]\n",
    "    y_true_flat = np.concatenate(y_true_node)\n",
    "    # Calculate accuracy score\n",
    "    accuracy_node = accuracy_score(y_true_flat, y_pred_binary)\n",
    "    print('F1 score reactive atom task:', f1_score(y_true_flat,np.array(y_pred_binary,dtype=np.float32), average='macro'))\n",
    "\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print('epoch {:d}/{:d},train_acc_classification {:.4f},train_node_acc {:.4f},train_loss {:.4f},train_node_loss {:.4f}'.format(\n",
    "            epoch + 1, n_epochs, accuracy,accuracy_node, total_loss, total_loss_node))\n",
    "    return accuracy, total_loss, labels, prediction1, y_true_node, y_pred_node, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a93145b5-ad19-4f5e-8c06-01ddeb1bab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_a_valid_epoch(n_epochs, epoch, model, data_loader, loss_criterion1, loss_criterion2):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    loss_node_app = []\n",
    "    loss_graph_radomize_app = []\n",
    "    y_true_node = []\n",
    "    y_pred_node = []\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            \n",
    "            smiles, bg, labels = batch_data\n",
    "            \n",
    "            bg = bg.to(device)\n",
    "            labels = labels.to(device)\n",
    "            n_feats_w_l = bg.ndata.pop('hv').to(device)\n",
    "            e_feats_ = bg.edata.pop('he').to(device)\n",
    "            n_feats_ = n_feats_w_l[:,:n_feats]\n",
    "            prediction1, prediction2, graph_feat = model(bg, n_feats_, e_feats_)\n",
    "            n_labels = n_feats_w_l[:,n_feats].unsqueeze(1)\n",
    "        \n",
    "            # Calculate the weights\n",
    "            counts = torch.bincount(n_labels.view(-1).long())\n",
    "            class_weights = 1.0 / counts.float()\n",
    "            class_weights = class_weights / class_weights.sum()\n",
    "            \n",
    "            loss_graph = loss_fn_graph(prediction1, labels.squeeze(1).long())\n",
    "            loss_node = weighted_binary_cross_entropy(prediction2,n_labels ,class_weights) #class_weights\n",
    "\n",
    "            loss = loss_graph + loss_node \n",
    "            \n",
    "            losses.append(loss_graph.data.item())\n",
    "            loss_node_app.append(loss_node.data.item())\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(prediction1.detach().cpu().numpy())\n",
    "        \n",
    "            y_true_node.extend(n_labels.cpu().numpy())\n",
    "            y_pred_node.extend(prediction2.detach().cpu().numpy())\n",
    "\n",
    "    \n",
    "    total_loss = np.mean(losses)\n",
    "    total_loss_node = np.mean(loss_node_app)\n",
    "    total_loss_graph_random = np.mean(loss_graph_radomize_app)\n",
    "    accuracy = accuracy_score(y_true, np.argmax(y_pred, axis=1))\n",
    "    print('F1 score classification task:', f1_score(y_true,np.argmax(y_pred, axis=1), average='macro'))\n",
    "\n",
    "    # Threshold for binary prediction\n",
    "    threshold_1 = 0.5\n",
    "    # Convert predicted probabilities to binary values\n",
    "    y_pred_binary = [1 if pred >= threshold_1 else 0 for pred in np.concatenate(y_pred_node)]\n",
    "    y_true_flat = np.concatenate(y_true_node)\n",
    "    # Calculate accuracy score\n",
    "    accuracy_node = accuracy_score(y_true_flat, y_pred_binary)\n",
    "    print('F1 score reactive atom task:', f1_score(y_true_flat,np.array(y_pred_binary,dtype=np.float32), average='macro'))\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print('epoch {:d}/{:d},valid_acc_classification {:.4f}, valid_node_acc {:.4f},valid_loss {:.4f},valid_node_loss {:.4f}'.format(\n",
    "            epoch + 1, n_epochs, accuracy, accuracy_node, total_loss, total_loss_node))\n",
    "    return accuracy, total_loss, labels, prediction1, y_true_node, y_pred_node, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "371a0a84-1022-405a-8e45-1a92a0cf3da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score classification task: 0.8474743598379123\n",
      "F1 score reactive atom task: 0.5779244759219162\n",
      "epoch 1/5,train_acc_classification 0.8731,train_node_acc 0.7493,train_loss 0.3512,train_node_loss 0.0455\n",
      "F1 score classification task: 1.0\n",
      "F1 score reactive atom task: 0.7857063021831172\n",
      "epoch 1/5,valid_acc_classification 1.0000, valid_node_acc 0.9280,valid_loss 0.0006,valid_node_loss 0.0177\n",
      "F1 score classification task: 0.9853952685086285\n",
      "F1 score reactive atom task: 0.8427954227755883\n",
      "epoch 2/5,train_acc_classification 0.9967,train_node_acc 0.9533,train_loss 0.0176,train_node_loss 0.0134\n",
      "F1 score classification task: 0.9999181805540261\n",
      "F1 score reactive atom task: 0.8476891967809697\n",
      "epoch 2/5,valid_acc_classification 0.9999, valid_node_acc 0.9555,valid_loss 0.0030,valid_node_loss 0.0183\n",
      "F1 score classification task: 0.9999880434324548\n",
      "F1 score reactive atom task: 0.8595148759211537\n",
      "epoch 3/5,train_acc_classification 1.0000,train_node_acc 0.9595,train_loss 0.0006,train_node_loss 0.0103\n",
      "F1 score classification task: 1.0\n",
      "F1 score reactive atom task: 0.8687022775235325\n",
      "epoch 3/5,valid_acc_classification 1.0000, valid_node_acc 0.9629,valid_loss 0.0001,valid_node_loss 0.0083\n",
      "F1 score classification task: 1.0\n",
      "F1 score reactive atom task: 0.8706859806315292\n",
      "epoch 4/5,train_acc_classification 1.0000,train_node_acc 0.9636,train_loss 0.0001,train_node_loss 0.0082\n",
      "F1 score classification task: 1.0\n",
      "F1 score reactive atom task: 0.8697316481598176\n",
      "epoch 4/5,valid_acc_classification 1.0000, valid_node_acc 0.9633,valid_loss 0.0000,valid_node_loss 0.0079\n",
      "F1 score classification task: 1.0\n",
      "F1 score reactive atom task: 0.871398679412163\n",
      "epoch 5/5,train_acc_classification 1.0000,train_node_acc 0.9638,train_loss 0.0000,train_node_loss 0.0078\n",
      "F1 score classification task: 1.0\n",
      "F1 score reactive atom task: 0.8699504069329564\n",
      "epoch 5/5,valid_acc_classification 1.0000, valid_node_acc 0.9634,valid_loss 0.0000,valid_node_loss 0.0077\n",
      "time required: 4.636911503473917\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "st_time = time.time()\n",
    "stopper = EarlyStopping(mode='higher', patience=5)\n",
    "n_epochs = 5\n",
    "for e in range(n_epochs):\n",
    "    accuracy, total_loss, labels, prediction, y_true_node, y_pred_node, train_model= run_a_train_epoch(n_epochs, e, model, train_loader, loss_fn_graph, loss_fn_node, optimizer)\n",
    "    accuracy_, total_loss_, labels_, prediction_, y_true_node_, y_pred_node_, train_model_= run_a_valid_epoch(n_epochs, e, model, valid_loader, loss_fn_graph, loss_fn_node)\n",
    "\n",
    "    #fn = 'model_' + str(e)\n",
    "        #torch.save(train_model.state_dict(), fn)\n",
    "en_time = time.time()\n",
    "print('time required:', (en_time-st_time)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4827fed0-33b2-4668-8d1b-b9771b862f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score classification task: 1.0\n",
      "F1 score reactive atom task: 0.8712175687627253\n",
      "epoch 2/1,valid_acc_classification 1.0000, valid_node_acc 0.9639,valid_loss 0.0000,valid_node_loss 0.0076\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy calculation\n",
    "accuracy_, total_loss_, labels_, prediction_, y_true_node_, y_pred_node_, train_model_= run_a_valid_epoch(1, 1, model, test_loader, loss_fn_graph, loss_fn_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bea46cf9-2d79-4528-b742-cb1ceecb0f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score classification task: 0.9937953987601318\n",
      "F1 score reactive atom task: 0.8438283029242029\n",
      "epoch 2/1,valid_acc_classification 0.9956, valid_node_acc 0.9493,valid_loss 0.0300,valid_node_loss 0.0212\n"
     ]
    }
   ],
   "source": [
    "# OOD accuracy calculation\n",
    "accuracy_, total_loss_, labels_, prediction_, y_true_node_, y_pred_node_, train_model_= run_a_valid_epoch(1, 1, model, ood_loader, loss_fn_graph, loss_fn_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58677a2a-502a-4686-8ba6-aa960fc4156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to save your model run this cell\n",
    "# fn = 'final_trained_ReactAIvate_model'\n",
    "# torch.save(model.state_dict(), fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08195a2f-afe8-43e4-9044-6c12759589bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981b7c40-3751-4e97-beca-6ce68f68ab41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871fe1a-add6-475d-af76-a482cde595a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReactAIvate (conda)",
   "language": "python",
   "name": "reactaivate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
